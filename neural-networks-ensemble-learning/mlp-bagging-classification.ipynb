{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9041f5bc",
   "metadata": {},
   "source": [
    "### Ambre JACQUOT et Yan WU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e3bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=10000,suppress=True)\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e7441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fcc6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f2e20",
   "metadata": {},
   "source": [
    "# I.Importation de la base de données 'Iris.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f58240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3  4\n",
      "0    5.1  3.5  1.4  0.2  1\n",
      "1    4.9  3.0  1.4  0.2  1\n",
      "2    4.7  3.2  1.3  0.2  1\n",
      "3    4.6  3.1  1.5  0.2  1\n",
      "4    5.0  3.6  1.4  0.2  1\n",
      "..   ...  ...  ...  ... ..\n",
      "145  6.7  3.0  5.2  2.3  3\n",
      "146  6.3  2.5  5.0  1.9  3\n",
      "147  6.5  3.0  5.2  2.0  3\n",
      "148  6.2  3.4  5.4  2.3  3\n",
      "149  5.9  3.0  5.1  1.8  3\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_iris = pd.read_csv('iris.txt',header=None,sep='\\t') \n",
    "print(df_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ab2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met tous les individus avec les 4 premières variables dans X et la dernière colonne dans y\n",
    "y=df_iris.iloc[:,4]\n",
    "X=df_iris.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476936e5",
   "metadata": {},
   "source": [
    "# II. Découpage de la base en Apprentissage/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef31b1a",
   "metadata": {},
   "source": [
    "Dans cette section, on crée une fonction découpage car elle sera réutilisée pour les autres datasets de la question 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9c7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def découpage(df, taille):\n",
    "    \"\"\" Permet de découper la base en apprentissage et en test\n",
    "    Parameters : \n",
    "    df : DataFrame\n",
    "    taille : float\n",
    "    Returns : \n",
    "        X_A, X_T : matrix\n",
    "        y_A, y_T : vector\n",
    "    \"\"\" \n",
    "    X_A=[]\n",
    "    y_A=[]\n",
    "    X_T=[]\n",
    "    y_T=[]\n",
    "    for classes in np.unique(df.iloc[:,-1]): # pour chaque classe on fait cette boucle\n",
    "        df_classe=df[df.iloc[:,-1]==classes] # on ne garde que la classe en cours\n",
    "        df_classe=df_classe.sample(frac=1,random_state=42) # on mélange les lignes\n",
    "        nombre=int(len(df_classe)*taille) # pour ne prendre que 2/3 de la classe \n",
    "        apprentissage=df_classe.iloc[:nombre] # sélection de 2/3\n",
    "        test=df_classe.iloc[nombre:] # sélection de 1/3\n",
    "        X_A.append(apprentissage.iloc[:,:-1])\n",
    "        y_A.append(apprentissage.iloc[:,-1])\n",
    "        X_T.append(test.iloc[:,:-1])\n",
    "        y_T.append(test.iloc[:,-1])\n",
    "# On réinitialise les index \n",
    "    X_A=pd.concat(X_A).reset_index(drop=True)\n",
    "    y_A=pd.concat(y_A).reset_index(drop=True)\n",
    "    X_T=pd.concat(X_T).reset_index(drop=True)\n",
    "    y_T=pd.concat(y_T).reset_index(drop=True)\n",
    "    return X_A, y_A, X_T, y_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d26600",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A, y_A, X_T, y_T = découpage(df_iris, 2/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd2fff",
   "metadata": {},
   "source": [
    "# III. Implémentation d'un perceptron multi-classe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a52c8",
   "metadata": {},
   "source": [
    "Ici, on va définir une fonction du perceptron multi-classes car elle sera réutilisée pour les autres Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85f2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_Multi_classe(df,X_A,y_A,X_T,y_T,epochs=2000, eps=10):\n",
    "    \"\"\" fonction qui programme le perceptron multi-classe\n",
    "    Parameters : \n",
    "    df : DataFrame\n",
    "    X_A, X_T : matrix\n",
    "    y_A, y_T : vector\n",
    "    epochs : int\n",
    "    eps : float\n",
    "    Returns : \n",
    "        X_test, X_a : matrix\n",
    "        y_test_encode, y_pred, y_encode, : vector\n",
    "    \"\"\" \n",
    "    X_a=X_A.to_numpy() #pour passer de dataframe à du numpy\n",
    "    y_a=y_A.to_numpy()\n",
    "    classes = np.unique(y_a)\n",
    "    index = {c: i for i, c in enumerate(classes)}\n",
    "    y_encode = np.array([index[c] for c in y_a])\n",
    "    #version numérique de y \n",
    "    nb_classes=len(classes)\n",
    "    nb_caractéristiques=X_a.shape[1]\n",
    "    W=np.zeros((nb_classes,nb_caractéristiques)) #matrice de poids\n",
    "    epoch=2000\n",
    "    eps=10\n",
    "    #Entrainement du perceptron\n",
    "    for i in range(epoch):\n",
    "        indices = np.arange(len(X_a))\n",
    "        np.random.shuffle(indices)\n",
    "        erreur=0\n",
    "        for j in indices:\n",
    "            xi = X_a[j]\n",
    "            yi = y_encode[j]\n",
    "            scores=W@xi\n",
    "            pred=np.argmax(scores)\n",
    "            if pred!=yi:\n",
    "                W[yi] += eps * xi\n",
    "                W[pred] -= eps * xi\n",
    "            if pred != yi:\n",
    "                erreur += 1\n",
    "\n",
    "    #Prédiction sur les données de test\n",
    "    X_test=X_T.to_numpy()\n",
    "    y_test=y_T.to_numpy()\n",
    "    y_test_encode=np.array([index[c] for c in y_test])\n",
    "    y_pred=[]\n",
    "    for xi in X_test:\n",
    "        scores=W@xi\n",
    "        y_pred.append(np.argmax(scores))\n",
    "    return y_test_encode, y_pred, y_encode, X_test, X_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f36608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encode, y_pred,y_encode, X_test,X_a = perceptron_Multi_classe(df_iris,X_A,y_A,X_T,y_T,epochs=500, eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f4116",
   "metadata": {},
   "source": [
    "# IV. Evaluation des performances du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c4c38",
   "metadata": {},
   "source": [
    "Dans cette section, on crée une fonction afficher_infos car elle sera réutilisée pour afficher l'évaluation des performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3096ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_infos(y_test_encode,y_pred):\n",
    "    \"\"\" Permet d'afficher la matrice de confusion, l'accuracy globale, la précision et le rappel par classe associé au modèle\n",
    "    Parameters : \n",
    "    y_test : vector\n",
    "    y_pred : vector\n",
    "    Returns : \n",
    "        None\n",
    "    \"\"\" \n",
    "    #Matrice de confusion\n",
    "    cm = confusion_matrix(y_test_encode, y_pred)\n",
    "    print(\"Matrice de confusion :\\n\", cm)\n",
    "\n",
    "    #Accuracy globale\n",
    "    accuracy = accuracy_score(y_test_encode, y_pred)\n",
    "    print(\"Accuracy globale :\", accuracy)\n",
    "\n",
    "    #Précision\n",
    "    precision = precision_score(y_test_encode, y_pred, average=None)\n",
    "    print(\"Précision par classe :\", precision)\n",
    "\n",
    "    #Rappel par classe\n",
    "    recall = recall_score(y_test_encode, y_pred, average=None)\n",
    "    print(\"Rappel par classe :\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2091abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[17  0  0]\n",
      " [ 0 14  3]\n",
      " [ 0  0 17]]\n",
      "Accuracy globale : 0.9411764705882353\n",
      "Précision par classe : [1.   1.   0.85]\n",
      "Rappel par classe : [1.         0.82352941 1.        ]\n"
     ]
    }
   ],
   "source": [
    "afficher_infos(y_test_encode,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47a728",
   "metadata": {},
   "source": [
    "# V. Implémentation d'un perceptron multi-couches avec 3 couches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f0078",
   "metadata": {},
   "source": [
    "Version sans normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561f56ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_A, y_encode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b724e39",
   "metadata": {},
   "source": [
    "# VI. Evaluation des performances du modèle MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5331473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[17  0  0]\n",
      " [ 0 14  3]\n",
      " [ 0  0 17]]\n",
      "Accuracy globale : 0.9411764705882353\n",
      "Précision par classe : [1.   1.   0.85]\n",
      "Rappel par classe : [1.         0.82352941 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_test)\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04c804",
   "metadata": {},
   "source": [
    "### Version avec normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7f1b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[17  0  0]\n",
      " [ 1 13  3]\n",
      " [ 0  0 17]]\n",
      "Accuracy globale : 0.9215686274509803\n",
      "Précision par classe : [0.94444444 1.         0.85      ]\n",
      "Rappel par classe : [1.         0.76470588 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_A_normal = scaler.fit_transform(X_A)\n",
    "X_T_normal = scaler.transform(X_T)\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_A_normal, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_T_normal)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645378f7",
   "metadata": {},
   "source": [
    "On remarque que le fait de normaliser les données fait perdre un petit peu de précision globale. \\\n",
    "En effet quand on regarde la matrice de confusion, on se rend compte qu'avec la normalisation, il se trompe une fois de plus de classe (une fois la classe 1 est mise dans la classe 0) alors que cela n'arrive pas dans les deux premiers modèles codés qui sont similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb15701",
   "metadata": {},
   "source": [
    "### Réapplication de toute la chaîne de traitement sur différentes bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbb354",
   "metadata": {},
   "source": [
    "#### Sur le Dataset : glass.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083fdf2",
   "metadata": {},
   "source": [
    "On commence par importer le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "278d3dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0      1     2     3      4     5     6     7    8  9\n",
      "0    1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.00  0.0  1\n",
      "1    1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.00  0.0  1\n",
      "2    1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.00  0.0  1\n",
      "3    1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.00  0.0  1\n",
      "4    1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.00  0.0  1\n",
      "..       ...    ...   ...   ...    ...   ...   ...   ...  ... ..\n",
      "209  1.51623  14.14  0.00  2.88  72.61  0.08  9.18  1.06  0.0  7\n",
      "210  1.51685  14.92  0.00  1.99  73.06  0.00  8.40  1.59  0.0  7\n",
      "211  1.52065  14.36  0.00  2.02  73.42  0.00  8.44  1.64  0.0  7\n",
      "212  1.51651  14.38  0.00  1.94  73.61  0.00  8.48  1.57  0.0  7\n",
      "213  1.51711  14.23  0.00  2.08  73.36  0.00  8.62  1.67  0.0  7\n",
      "\n",
      "[214 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "df_glass = pd.read_csv('glass.txt',header=None,sep='\\s+') \n",
    "print(df_glass)\n",
    "y=df_glass.iloc[:,9]\n",
    "X=df_glass.iloc[:,0:9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90e7a5",
   "metadata": {},
   "source": [
    "On fait un découpage de la base en Apprentissage/Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13c3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_A_glass, y_A_glass, X_T_glass, y_T_glass = découpage(df_glass, 2/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c44fab",
   "metadata": {},
   "source": [
    "On implémente un perceptron multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "106f2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[ 7 17  0  0  0  0]\n",
      " [ 6 20  0  0  0  0]\n",
      " [ 2  4  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  2  0  0  1  0]\n",
      " [ 0  0  0  5  1  4]]\n",
      "Accuracy globale : 0.43243243243243246\n",
      "Précision par classe : [0.46666667 0.41666667 0.         0.         0.5        1.        ]\n",
      "Rappel par classe : [0.29166667 0.76923077 0.         0.         0.33333333 0.4       ]\n"
     ]
    }
   ],
   "source": [
    "y_test_encode, y_pred, y_encode, X_test,X_a=perceptron_Multi_classe(df_glass,X_A_glass, y_A_glass, X_T_glass, y_T_glass)\n",
    "afficher_infos(y_test_encode,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bf4ad",
   "metadata": {},
   "source": [
    "On implémente maintenant le perceptron multi-couches d'abord sans normaliser les données puis en les normalisant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39c4ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[ 0 24  0  0  0  0]\n",
      " [ 4 22  0  0  0  0]\n",
      " [ 0  6  0  0  0  0]\n",
      " [ 4  1  0  0  0  0]\n",
      " [ 2  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 10]]\n",
      "Accuracy globale : 0.43243243243243246\n",
      "Précision par classe : [0.         0.41509434 0.         0.         0.         0.90909091]\n",
      "Rappel par classe : [0.         0.84615385 0.         0.         0.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Version sans normalisation\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=5000, random_state=42)\n",
    "mlp.fit(X_A_glass, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_test)\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30446347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[21  3  0  0  0  0]\n",
      " [15 10  0  0  0  1]\n",
      " [ 3  3  0  0  0  0]\n",
      " [ 0  2  0  3  0  0]\n",
      " [ 0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0 10]]\n",
      "Accuracy globale : 0.5945945945945946\n",
      "Précision par classe : [0.53846154 0.55555556 0.         1.         0.         0.71428571]\n",
      "Rappel par classe : [0.875      0.38461538 0.         0.6        0.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Version avec normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_A_normal = scaler.fit_transform(X_a)\n",
    "X_T_normal = scaler.transform(X_test)\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=2000, random_state=42)\n",
    "mlp.fit(X_A_normal, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_T_normal)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c213f",
   "metadata": {},
   "source": [
    "Ici, on remarque la normalisation des données augmente significativement la précision globale comparé aux autres modèles. \\\n",
    "On a gardé trois neurones et on observe une vraie différence entre la normalisation et sans. Le modèle multi-classe codé au début s'approche de celui sans normalisation mais ils ne sont pas très bon tous les deux. \\\n",
    "Cet exemple montre l'importance de normaliser les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf982e",
   "metadata": {},
   "source": [
    "#### Sur le Dataset : breast-cancer-wisconsin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9055a5",
   "metadata": {},
   "source": [
    "On commence par importer le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be058410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0  1  2   3  4   5   6  7  8  9\n",
      "0     5  1  1   1  2   1   3  1  1  1\n",
      "542   5  3  1   1  2   1   1  1  1  1\n",
      "338   1  1  1   1  1   1   2  1  1  1\n",
      "337   1  1  1   1  2   1   3  1  1  1\n",
      "543   4  1  1   1  2   1   2  1  1  1\n",
      "..   .. .. ..  .. ..  ..  .. .. .. ..\n",
      "282   1  4  3  10  4  10   5  6  1  2\n",
      "279  10  5  7   3  3   7   3  3  8  2\n",
      "273   7  2  4   1  3   4   3  3  1  2\n",
      "270   8  4  7   1  3  10   3  9  2  2\n",
      "698   4  8  8   5  4   5  10  4  1  2\n",
      "\n",
      "[699 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "df_bcw = pd.read_csv('breast-cancer-wisconsin.txt',header=None,sep='\\t') \n",
    "print(df_bcw.sort_values(by=df_bcw.columns[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9593fc7",
   "metadata": {},
   "source": [
    "On fait un découpage de la base en Apprentissage/Test et on implémente un perceptron multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8527f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[140  13]\n",
      " [ 22  59]]\n",
      "Accuracy globale : 0.8504273504273504\n",
      "Précision par classe : [0.86419753 0.81944444]\n",
      "Rappel par classe : [0.91503268 0.72839506]\n"
     ]
    }
   ],
   "source": [
    "X_A_bcw, y_A_bcw, X_T_bcw, y_T_bcw = découpage(df_bcw, 2/3)\n",
    "y_test_encode, y_pred, y_encode, X_test,X_a=perceptron_Multi_classe(df_bcw,X_A_bcw, y_A_bcw, X_T_bcw, y_T_bcw)\n",
    "afficher_infos(y_test_encode,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fcefb",
   "metadata": {},
   "source": [
    "On implémente maintenant le perceptron multi-couches d'abord sans normaliser les données puis en les normalisant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b29c3974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[148   5]\n",
      " [  4  77]]\n",
      "Accuracy globale : 0.9615384615384616\n",
      "Précision par classe : [0.97368421 0.93902439]\n",
      "Rappel par classe : [0.96732026 0.95061728]\n"
     ]
    }
   ],
   "source": [
    "# Version sans normalisation\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=5000, random_state=42)\n",
    "mlp.fit(X_a, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_test)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe4df63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[146   7]\n",
      " [  1  80]]\n",
      "Accuracy globale : 0.9658119658119658\n",
      "Précision par classe : [0.99319728 0.91954023]\n",
      "Rappel par classe : [0.95424837 0.98765432]\n"
     ]
    }
   ],
   "source": [
    "# Version avec normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_A_normal = scaler.fit_transform(X_a)\n",
    "X_T_normal = scaler.transform(X_test)\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=2000, random_state=42)\n",
    "mlp.fit(X_A_normal, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_T_normal)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8b051",
   "metadata": {},
   "source": [
    "Ici on voit une amélioration croissante entre les modèles. Celui avec la normalisation est légèrement meilleur et on pourrait voir une certaine stabilisation.\\\n",
    "Le percetron multi-classes n'est pas aussi bon en précision mais reste tout de même bon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb951245",
   "metadata": {},
   "source": [
    "#### Sur le Dataset : Lsun.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579a22b",
   "metadata": {},
   "source": [
    "On commence par importer le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c3a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1  2\n",
      "0    3.277701  0.814082  1\n",
      "1    0.387577  0.176780  1\n",
      "2    0.268546  0.582963  1\n",
      "3    2.031145  0.244597  1\n",
      "4    0.188677  0.461280  1\n",
      "..        ...       ... ..\n",
      "395  2.636088  2.056805  3\n",
      "396  2.938300  2.321199  3\n",
      "397  3.080906  2.209603  3\n",
      "398  2.404517  2.641618  3\n",
      "399  3.248655  2.297291  3\n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_Lsun = pd.read_csv('Lsun.txt',header=None,sep='\\t')\n",
    "print(df_Lsun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b13ee",
   "metadata": {},
   "source": [
    "On fait un découpage de la base en Apprentissage/Test et on implémente un perceptron multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ef0ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[46  3 18]\n",
      " [ 0 27  7]\n",
      " [ 0  0 34]]\n",
      "Accuracy globale : 0.7925925925925926\n",
      "Précision par classe : [1.         0.9        0.57627119]\n",
      "Rappel par classe : [0.68656716 0.79411765 1.        ]\n"
     ]
    }
   ],
   "source": [
    "X_A_Lsun, y_A_Lsun, X_T_Lsun, y_T_Lsun = découpage(df_Lsun, 2/3)\n",
    "y_test_encode, y_pred, y_encode, X_test,X_a=perceptron_Multi_classe(df_Lsun, X_A_Lsun, y_A_Lsun, X_T_Lsun, y_T_Lsun)\n",
    "afficher_infos(y_test_encode,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa7a64",
   "metadata": {},
   "source": [
    "On implémente maintenant le perceptron multi-couches d'abord sans normaliser les données puis en les normalisant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab27800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[67  0  0]\n",
      " [ 1 33  0]\n",
      " [ 0  0 34]]\n",
      "Accuracy globale : 0.9925925925925926\n",
      "Précision par classe : [0.98529412 1.         1.        ]\n",
      "Rappel par classe : [1.         0.97058824 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Version sans normalisation\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=5000, random_state=42)\n",
    "mlp.fit(X_a, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_test)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40a4cf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[67  0  0]\n",
      " [ 0 34  0]\n",
      " [ 0  0 34]]\n",
      "Accuracy globale : 1.0\n",
      "Précision par classe : [1. 1. 1.]\n",
      "Rappel par classe : [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Version avec normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_A_normal = scaler.fit_transform(X_a)\n",
    "X_T_normal = scaler.transform(X_test)\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=2000, random_state=42)\n",
    "mlp.fit(X_A_normal, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_T_normal)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef4051",
   "metadata": {},
   "source": [
    "On observe la même chose que sur le dataset précédent en terme de précision globale.\\ \n",
    "La version avec normalisation a une précision à 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb853df9",
   "metadata": {},
   "source": [
    "#### Sur le Dataset : Wave.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e862b",
   "metadata": {},
   "source": [
    "On commence par importer le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14da1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0     1     2     3     4     5     6     7     8     9   ...    31  \\\n",
      "4999  2.05 -1.99  1.66  2.18  2.22  2.53  3.09  2.20  1.42  0.62  ...  0.28   \n",
      "4371  1.33 -0.02  0.48  1.60  3.11  3.58  3.00  1.69  2.13  0.00  ... -1.09   \n",
      "1998 -1.25  2.28 -0.34  1.26  2.93  2.15  2.05  1.23  2.15  1.10  ...  1.62   \n",
      "1997  1.27  0.68 -0.12  2.05  3.76  6.14  7.73  5.45  3.50  1.53  ...  0.57   \n",
      "1994  0.90 -0.16 -0.63  0.91  0.35 -0.04  1.18  0.61  0.25  2.27  ... -0.13   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "2573 -1.96 -0.82 -0.99  0.04 -1.61  0.02  1.36  2.34  1.97  4.72  ... -0.01   \n",
      "2578  1.02  1.22  2.63  1.30 -0.00  0.65  1.38  1.62 -0.71  3.02  ...  1.81   \n",
      "2582 -0.36 -0.25 -0.44 -0.80 -0.81 -0.07  0.70 -0.20  2.44  3.47  ... -0.72   \n",
      "2563  1.11 -0.17  0.13  0.27 -0.04  1.23  0.87 -0.80  0.50  3.21  ... -0.94   \n",
      "2499  0.98  0.98 -0.20 -0.66  1.31  0.74 -0.84  2.73  0.56  1.37  ... -0.56   \n",
      "\n",
      "        32    33    34    35    36    37    38    39  40  \n",
      "4999 -0.77  1.39  0.51  0.24 -1.13  1.01  0.06  0.61   0  \n",
      "4371  0.48 -1.35  1.06  0.43  0.08 -0.93 -0.19  1.21   0  \n",
      "1998 -0.06  0.10  0.05 -0.60  0.59 -0.48 -0.60  0.33   0  \n",
      "1997 -0.89  0.24 -0.30  0.53  0.56 -0.52 -0.78  1.01   0  \n",
      "1994  0.10 -0.69 -2.42  0.74  0.54  1.32 -0.95  0.29   0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...  ..  \n",
      "2573  1.03 -0.81 -0.39  0.39 -1.92 -2.35  0.56 -0.05   2  \n",
      "2578 -0.69  1.38  0.08 -0.79 -0.88 -1.01 -0.12 -0.50   2  \n",
      "2582  0.03 -1.43 -0.17 -1.70  2.20 -0.09  0.67  0.32   2  \n",
      "2563  0.71  1.99  0.10 -0.30  0.59 -0.85 -1.34 -0.31   2  \n",
      "2499  1.25  0.46 -1.06  0.80  0.86  0.21  1.50 -1.77   2  \n",
      "\n",
      "[5000 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "df_wave = pd.read_csv('Wave.txt',header=None,sep='\\s+') \n",
    "print(df_wave.sort_values(by=df_wave.columns[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff297a6",
   "metadata": {},
   "source": [
    "On fait un découpage de la base en Apprentissage/Test et on implémente un perceptron multi-classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59c3d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[419 125  20]\n",
      " [  6 543   2]\n",
      " [100 119 333]]\n",
      "Accuracy globale : 0.7768446310737852\n",
      "Précision par classe : [0.79809524 0.68996188 0.93802817]\n",
      "Rappel par classe : [0.7429078  0.98548094 0.60326087]\n"
     ]
    }
   ],
   "source": [
    "X_A_wave, y_A_wave, X_T_wave, y_T_wave = découpage(df_wave, 2/3)\n",
    "y_test_encode, y_pred, y_encode, X_test,X_a=perceptron_Multi_classe(df_wave, X_A_wave, y_A_wave, X_T_wave, y_T_wave)\n",
    "afficher_infos(y_test_encode,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4971e",
   "metadata": {},
   "source": [
    "On implémente maintenant le perceptron multi-couches d'abord sans normaliser les données puis en les normalisant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02ff059d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[470  46  48]\n",
      " [ 39 481  31]\n",
      " [ 44  32 476]]\n",
      "Accuracy globale : 0.8560287942411517\n",
      "Précision par classe : [0.84990958 0.86046512 0.85765766]\n",
      "Rappel par classe : [0.83333333 0.87295826 0.86231884]\n"
     ]
    }
   ],
   "source": [
    "# Version sans normalisation\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=5000, random_state=42)\n",
    "mlp.fit(X_a, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_test)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65105d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[484  37  43]\n",
      " [ 39 484  28]\n",
      " [ 46  34 472]]\n",
      "Accuracy globale : 0.8638272345530894\n",
      "Précision par classe : [0.85061511 0.87207207 0.86924494]\n",
      "Rappel par classe : [0.85815603 0.8784029  0.85507246]\n"
     ]
    }
   ],
   "source": [
    "# Version avec normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_A_normal = scaler.fit_transform(X_a)\n",
    "X_T_normal = scaler.transform(X_test)\n",
    "\n",
    "mlp=MLPClassifier(hidden_layer_sizes=(3,), max_iter=2000, random_state=42)\n",
    "mlp.fit(X_A_normal, y_encode)\n",
    "# Prédictions sur les données de test\n",
    "y_pred_mlp=mlp.predict(X_T_normal)\n",
    "\n",
    "afficher_infos(y_test_encode,y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc891d",
   "metadata": {},
   "source": [
    "On observe ici la même chose en terme de précision que sur les dataset précédents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a89a9",
   "metadata": {},
   "source": [
    "On peut donc conclure avec nos tests que la normalisation permet très souvent d'avoir une meilleure précision et une meilleure matrice de confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aad013",
   "metadata": {},
   "source": [
    "# VII.Bagging de réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493f5f0-7303-44d6-89e4-7241a5d2d952",
   "metadata": {},
   "source": [
    "Dans cette section, on crée une fonction bagging et on va tester tous les bases avec cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a43f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "def bagging (X_A,y_A,X_T,K,N,M):\n",
    "    \"\"\" Fonction qui implémente la méthode de Bagging (Bootstrap Aggregating)  \n",
    "    Paramètres :\n",
    "    X_A : DataFrame \n",
    "        Les caractéristiques (features) des données d'apprentissage.\n",
    "    y_A : Series\n",
    "        Les étiquettes (labels) correspondantes des données d'apprentissage.\n",
    "    X_T : DataFrame \n",
    "        Les caractéristiques des données de test.\n",
    "    K : int\n",
    "        Le nombre d’échantillons bootstrap à créer.\n",
    "    N : int\n",
    "        Le nombre de neurones dans la couche cachée du MLP.\n",
    "    M : int\n",
    "        Le nombre maximal d’itérations pour l’apprentissage du MLP.\n",
    "    Retour :\n",
    "    final_prediction : array\n",
    "        Le vecteur des prédictions finales obtenu après agrégation (vote majoritaire).\n",
    "    \"\"\"\n",
    "    bootstrap = [] # Liste pour stocker les échantillons bootstrap\n",
    "    classifiers = [] # Liste pour stocker les modèles MLP entraînés\n",
    "    predictions = [] # Liste pour stocker les prédictions de chaque modèle\n",
    "\n",
    "    # créer K échantillons bootstrap sur la base d’apprentissage A \n",
    "    for i in range(K):\n",
    "        n = len(X_A)\n",
    "        # Tirage aléatoire avec remise (bootstrap)\n",
    "        indices = np.random.choice(n, size=n, replace=True)   \n",
    "        X = X_A.to_numpy()[indices]\n",
    "        y = y_A.to_numpy()[indices]\n",
    "        bootstrap.append((X, y))\n",
    "        \n",
    "    #créer un classifieur MLP sur chaque échantillon bootstrap.\n",
    "    for i,j in bootstrap:\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(N,),max_iter=M)\n",
    "        mlp.fit(i, j)\n",
    "        classifiers.append(mlp)\n",
    "        \n",
    "    #créer le modèle agrégé H à partir des K classifieurs appris dans b)\n",
    "    for i in classifiers:\n",
    "        pred = i.predict(X_T.to_numpy())\n",
    "        predictions.append(pred)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # prend la classe majoritaire (mode) pour chaque observation parmi les K prédictions\n",
    "    final_prediction = mode(predictions, axis=0).mode.flatten()\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0431b1",
   "metadata": {},
   "source": [
    "Testons le bagging de réseaux de neurones sur les précédents datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1d95b",
   "metadata": {},
   "source": [
    "#### Sur Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "463296e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[17  0  0]\n",
      " [ 0 14  3]\n",
      " [ 0  0 17]]\n",
      "Accuracy globale : 0.9411764705882353\n",
      "Précision par classe : [1.   1.   0.85]\n",
      "Rappel par classe : [1.         0.82352941 1.        ]\n"
     ]
    }
   ],
   "source": [
    "X_A, y_A, X_T, y_T = découpage(df_iris, 2/3)\n",
    "# Prédictions sur les données de test\n",
    "y_pred  = bagging (X_A,y_A,X_T,5,3,1000)\n",
    "afficher_infos(y_T,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15ee34-7831-4d62-9ad6-20541e8fde43",
   "metadata": {},
   "source": [
    "Le modèle présente une très bonne performance，seules quelques erreurs apparaissent pour la troisième classe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead64aa",
   "metadata": {},
   "source": [
    "#### Sur Glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e0a0878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[16  8  0  0  0  0]\n",
      " [ 8 18  0  0  0  0]\n",
      " [ 2  4  0  0  0  0]\n",
      " [ 0  5  0  0  0  0]\n",
      " [ 0  2  0  0  0  1]\n",
      " [ 0  0  0  0  0 10]]\n",
      "Accuracy globale : 0.5945945945945946\n",
      "Précision par classe : [0.61538462 0.48648649 0.         0.         0.         0.90909091]\n",
      "Rappel par classe : [0.66666667 0.69230769 0.         0.         0.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "X_A, y_A, X_T, y_T = découpage(df_glass, 2/3)\n",
    "# Prédictions sur les données de test\n",
    "y_pred  = bagging (X_A,y_A,X_T,15,10,2000)\n",
    "afficher_infos(y_T,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ce547-158c-4638-ac37-446534e00176",
   "metadata": {},
   "source": [
    "Le modèle obtient une accuracy globale faible,indiquant des difficultés à bien généraliser.Certaines classes ne sont pas reconnues, tandis que seule la sixième classe atteint une bonne performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa336a",
   "metadata": {},
   "source": [
    "#### Sur Breast-cancer-wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ecd5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[148   5]\n",
      " [  5  76]]\n",
      "Accuracy globale : 0.9572649572649573\n",
      "Précision par classe : [0.96732026 0.9382716 ]\n",
      "Rappel par classe : [0.96732026 0.9382716 ]\n"
     ]
    }
   ],
   "source": [
    "X_A, y_A, X_T, y_T = découpage(df_bcw, 2/3)\n",
    "# Prédictions sur les données de test\n",
    "y_pred  = bagging (X_A,y_A,X_T,10,10,1000)\n",
    "afficher_infos(y_T,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5978d1-f4bd-49c1-bcd7-7351d229b5d8",
   "metadata": {},
   "source": [
    "Le modèle présente bonnne performances avec une précision globale，Les deux classes sont bien distinguées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f1a49",
   "metadata": {},
   "source": [
    "#### Sur Lsun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6921a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[67  0  0]\n",
      " [ 0 34  0]\n",
      " [ 0  0 34]]\n",
      "Accuracy globale : 1.0\n",
      "Précision par classe : [1. 1. 1.]\n",
      "Rappel par classe : [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_A, y_A, X_T, y_T = découpage(df_Lsun, 2/3)\n",
    "# Prédictions sur les données de test\n",
    "y_pred  = bagging (X_A,y_A,X_T,10,10,1000)\n",
    "afficher_infos(y_T,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bd357-7c0c-4f33-86ca-438c4f7f6ced",
   "metadata": {},
   "source": [
    "Le modèle atteint une performance parfaite (100%) : toutes les classes sont correctement prédites, sans aucune erreur de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce838d11",
   "metadata": {},
   "source": [
    "#### Sur Wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b64ce144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      " [[484  37  43]\n",
      " [ 49 471  31]\n",
      " [ 47  30 475]]\n",
      "Accuracy globale : 0.8578284343131374\n",
      "Précision par classe : [0.83448276 0.87546468 0.86520947]\n",
      "Rappel par classe : [0.85815603 0.85480944 0.86050725]\n"
     ]
    }
   ],
   "source": [
    "X_A, y_A, X_T, y_T = découpage(df_wave, 2/3)\n",
    "# Prédictions sur les données de test\n",
    "y_pred  = bagging (X_A,y_A,X_T,5,10,1000)\n",
    "afficher_infos(y_T,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3ef91-a003-4375-9b8b-90ed6d0d7599",
   "metadata": {},
   "source": [
    "Le modèle présente une bonne performance globale, avec des précisions et rappels équilibrés entre les classes, indiquant une classification stable et homogène."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448e105-05b0-459d-8811-fc445bda2d94",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "Dans ce TP, nous avons implémenté une fonction de perceptron multi-classe ainsi qu’un modèle de bagging, et nous avons testé les cinq bases de données avec ces deux fonctions ainsi qu’avec un modèle MLP, sans et avec normalisation.\n",
    "\n",
    "Nous avons observé que les résultats pour les modèles étaient globalement satisfaisants, sauf pour la base Glass, où la performance est un peu moins bonne, probablement parce qu’elle contient plus de classes et que les données sont plus complexes. On remarque également que, pour la plupart des bases, les résultats avec normalisation sont meilleurs que sans normalisation, ce qui montre l’importance de normaliser les données avant l’apprentissage.\n",
    "\n",
    "Enfin, le modèle bagging obtient des performances très proches de celles du MLP avec normalisation, ce qui confirme que l’agrégation de plusieurs classifieurs peut stabiliser et améliorer légèrement les prédictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
